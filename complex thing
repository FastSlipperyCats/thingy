import gym
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

# Policy network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.softmax(self.fc3(x), dim=-1)

# Value network
class ValueNetwork(nn.Module):
    def __init__(self, state_dim):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# Proximal Policy Optimization (PPO) algorithm
class PPO:
    def __init__(self, state_dim, action_dim, lr=3e-4):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.value = ValueNetwork(state_dim)
        self.optimizer = optim.Adam(list(self.policy.parameters()) + list(self.value.parameters()), lr=lr)
        self.gamma = 0.99
        self.eps_clip = 0.2
        self.K_epochs = 10

    def select_action(self, state):
        state = torch.tensor(state, dtype=torch.float32)
        probs = self.policy(state)
        dist = Categorical(probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)

    def update(self, memory):
        rewards = []
        discounted_reward = 0
        for reward in reversed(memory.rewards):
            discounted_reward = reward + self.gamma * discounted_reward
            rewards.insert(0, discounted_reward)

        rewards = torch.tensor(rewards, dtype=torch.float32)
        log_probs = torch.stack(memory.log_probs)
        states = torch.stack(memory.states)
        actions = torch.tensor(memory.actions, dtype=torch.int64)

        for _ in range(self.K_epochs):
            state_values = self.value(states).squeeze()
            advantages = rewards - state_values.detach()
            old_log_probs = log_probs.detach()

            new_log_probs = self.policy(states).gather(1, actions.unsqueeze(1)).squeeze()
            ratio = torch.exp(new_log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages

            loss = -torch.min(surr1, surr2).mean() + 0.5 * ((rewards - state_values) ** 2).mean()

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

# Memory class
class Memory:
    def __init__(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.log_probs = []

    def clear_memory(self):
        del self.states[:]
        del self.actions[:]
        del self.rewards[:]
        del self.log_probs[:]

# Training loop
def train_agent(env_name='LunarLander-v2', num_episodes=1000):
    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    ppo = PPO(state_dim, action_dim)
    memory = Memory()

    for episode in range(num_episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            action, log_prob = ppo.select_action(state)
            next_state, reward, done, _ = env.step(action)

            memory.states.append(torch.tensor(state, dtype=torch.float32))
            memory.actions.append(action)
            memory.rewards.append(reward)
            memory.log_probs.append(log_prob)

            state = next_state
            total_reward += reward

        ppo.update(memory)
        memory.clear_memory()

        print(f"Episode {episode + 1}, Total Reward: {total_reward}")

train_agent()
